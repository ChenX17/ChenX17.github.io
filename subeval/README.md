# 文本质量主观评测网页

这是一个用于比较两个模型生成文本质量的主观评测工具，支持在线评分和结果下载。

## 功能特点

- 📊 **直观对比**: 并排显示两个模型的输出文本
- 🎯 **简单评分**: 三选一评分机制（模型A更好/模型B更好/两者相当）
- 📈 **实时进度**: 显示评测进度和完成状态
- 💾 **结果下载**: 支持下载详细的评测结果到txt文件
- 📱 **响应式设计**: 支持桌面和移动设备
- 🌐 **可分享**: 可以部署到网上供多人使用

## 使用方法

### 1. 准备数据文件

数据文件需要包含 `model_a` 和 `model_b` 两个字段，支持以下格式：

**JSONL格式** (推荐):
```jsonl
{"model_a": "模型A的输出文本", "model_b": "模型B的输出文本"}
{"model_a": "另一个样本的模型A输出", "model_b": "另一个样本的模型B输出"}
```

**JSON格式**:
```json
[
  {"model_a": "模型A的输出文本", "model_b": "模型B的输出文本"},
  {"model_a": "另一个样本的模型A输出", "model_b": "另一个样本的模型B输出"}
]
```

### 2. 打开网页

直接在浏览器中打开 `subjective_evaluation.html` 文件。

### 3. 上传数据

点击"📁 上传数据文件"按钮，选择准备好的JSON或JSONL文件。

### 4. 进行评测

- 逐个查看每个样本的模型A和模型B输出
- 为每个样本选择质量更好的模型：
  - **模型A更好**: 认为模型A的输出质量更高
  - **两者相当**: 认为两个模型输出质量相近
  - **模型B更好**: 认为模型B的输出质量更高

### 5. 下载结果

完成评测后，点击"📥 下载结果"按钮下载评测结果。结果文件包含：
- 评测统计信息
- 每个样本的详细评测结果
- 时间戳和元数据

## 部署到网上

### 方法1: GitHub Pages

1. 将文件上传到GitHub仓库
2. 在仓库设置中启用GitHub Pages
3. 分享生成的网页链接

### 方法2: 其他静态网站托管

可以使用以下平台托管：
- Netlify
- Vercel
- Firebase Hosting
- 阿里云OSS
- 腾讯云COS

只需上传 `subjective_evaluation.html` 文件即可。

## 示例数据

项目中包含了 `sample_data.jsonl` 示例文件，可以用来测试网页功能。

## 技术特点

- **纯前端实现**: 无需服务器，数据处理完全在浏览器中进行
- **数据安全**: 所有数据都在本地处理，不会上传到服务器
- **现代UI**: 使用CSS3和现代设计理念
- **兼容性好**: 支持主流浏览器

## 自定义修改

如需修改评分选项或界面样式，可以直接编辑HTML文件中的相应部分：

- 修改评分选项：编辑 `renderEvaluationItems()` 函数中的rating-options部分
- 修改样式：编辑 `<style>` 标签中的CSS
- 修改功能：编辑 `<script>` 标签中的JavaScript

## 注意事项

- 数据文件大小建议不超过10MB，以确保良好的加载性能
- 建议在评测过程中定期下载结果，避免意外丢失
- 如果需要多人协作评测，可以分别下载结果后合并分析

## 支持的浏览器

- Chrome 60+
- Firefox 55+
- Safari 12+
- Edge 79+

---

如有问题或建议，欢迎反馈！